#+title: SPNG Deconvolution

* Overview

The original signal processing (OSP) in WCT convolves the measured ADC waveforms
across adjacent channels with a kernel formed as the ratio of filters and
responses.  They are listed with their approximate sizes in bins of channel and
time samples.

- 2D field response (21, 200)
- 1D electronics response (1,20)
- 1D channel filter (21,1)
- 1D time filter (1, 100)  (two of them: "gauss" and "wiener")
- 1D "RC" aka long electronics response (1,10000)

To avoid any cyclic aliasing due to applying the Fourier method of convolution,
the waveform, response and filter arrays must all be padded in interval space to
a common larger value.  Each dimension size must be the sum of the corresponding
dimension size of its parts with one less for each element of the kernel.  This
is the usual "M+N-1" rule extended to multiple convolutions.  1D responses are
broadcast across their dimension of size 1 and should not be included in the
count.

Assuming a PDHD U plane of size (800,6000) and ignoring the "RC" response for
now, the padded dimension sizes would be:

- channel :: $800 + 21 + 21- 2 = 840$
- time :: $6000 + 200 + 20 + 100 - 3 = 6315$

The following graph represents most of the processing that must be done.

[[file:decon-flow.png]] 

It omits two important issues.

First, the 1D "RC" response deconvolution can be applied to the result.  As is
done in OSP, this requires the Fourier space signal "S" to be brought back to
interval space in order to pad it to accomodate the long RC response.  The RC
deconvolution then requires a large but 1D FFT round trip.  The RC could be
accommodated from the start but this would more than double the memory required.

The second omission is that OSP performs two deconvoutions each with a different
time filter ("gauss" and "wiener"), drawn as "Ft" in the diagram.  If redundant
FFTs are to be avoided, the program must retain two "F/R" tensors.

A comment on the long RC, the one relevant to MicroBooNE extends longer than the
nominal 3ms DUNE readout.  In streaming (SNB) mode, this will require the "tail"
of the signal processing to be snipped and added to more than one subsequent
chunk.

* Design considerations 

To map the computation to a WCT data flow graph we must consider some computational optimization issues.

First, the value of the "F/R" kernel depends on a few parameters that may vary over "space" and "time":

1. The kernel content and size depends on the anode plane (U, V or W).
2. The kernel content may depend on the anode.
3. The final shape is driven by (fixed) kernel size and potentially varying measure size.


In a single job, we can be assured that type 1 variation must be accommodated.
This requires a unique kernel to be brought to each node that processes data
from a given plane type.

Type 2 variation must be supported because current PDHD requires "bad" and
"nominal" FRs and because current job configurations will load and process all
four of the PDHD anodes in a single job.  This is done out of historical
inertia.  For future, in particularly for DUNE FD, there are technical arguments
to define per-anode jobs and these would not need to account for Type 2
variation.  However, other arguments favor multiple anodes processed in a
multi-threaded WCT graph when GPU resource management issues are included.

No case of Type 3 variation is currently known to exist.  However, there is a
strong likelihood that DUNE FD will be a source fo trigger records with at least
two sizes.  Or three when considering the SNB case.  SNB trigger records will
almost certainly be handled by special jobs so we ignore them here.  In "normal"
running, in a single "run", we may expect "short" low energy and "nominal"
length trigger records.

The variability of the kernel size over run time and the need to consume the
same kernel in different DFP graph nodes pose problems related thread safety and
minimizing memory usage and repeating computation.  Two options have been
identified:

1. User-configuration predetermines all possible kernel shapes.  The kernel
   source component pre-calculates all corresponding kernels and stores these in
   a map using shape as keys.  The map is held constant during subsequent calls
   from the multi-threaded context.  An error value (empty kernel) or exception
   is raised when a consumer requests and unknown shape.

   This option puts a burden on users but allows for simpler code and faster
   runtime while producing a less flexible DFP. 

2. The kernel source dynamically generates kernels of a given shape.  Each
   request must use a mutext to protect the entire request (checking cache,
   producing novel shaped kernel, filling cache), blocking all other concurrent
   requests.

   This option simplifies user configuration while making code more complicated
   and slower while producing a more flexible DFP.

On balance, option 2 will be followed.  If the performance is problematic and/or
the user configuration burden overestimated then option 1 can be provided at a
later time.

* Design

An ~ITorchSpectrum~ class ~DeconKernel~ (~SPNGDeconKernel~ DFP node type name) will be
implemented to provide the "F/R" kernel.  It's main method is:

#+begin_src cpp
    virtual torch::Tensor spectrum(const std::vector<int64_t> & measure_shape);
#+end_src

This is queried with the shape of measure tensor (ignoring batch dimension).
The returned tensor is of the *padded shape* larger than the measured shape used
in the query.

An ~ITorchTensorFilter~ class ~KernelConvolve~ (~SPNGKernelConvolve~ DFP node type
name) will be implemented to perform the following:

1. Query an ~ITorchSpectrum~ for the kernel using the input measure tensor's shape.
2. Use the returned kernel shape as a padding target to apply to the input measure tensor.
3. Apply forward 2D DFT to measure.
4. Multiply with kernel.
5. Apply inverse 2D DFT to measure.
6. Apply shifts.
7. Optionally crop.


Both components must collude on these points:

1. The query shape is that of the measure, the returned kernel is padded assuming the query shape.
2. The kernel provided in Fourier space representation.
3. The DFT ordering (eg, r2c rfft on time then c2c fft on channel dimensions vs symmetric c2c on both dimensions).


The crop is made optional so that ~KernelConvolve~ component can be used in both
windowed (nominal trigger records) and chunked-streaming (SNB) processing modes.

Note, this design is generic for application beyond just SP decon.  It can form
the core for an SPNG-style detector simulation.

Here is a variant flow graph showing this factoring.

[[file:decon-components.png]]

This figure draws out some design features.  First, the ~KernelConvolve~ node, as
the name implies, is generic.  It merely operates as a decon node if given a
decon kernel.

The ~DeconKernel~ factors into a "frontend" that constructs the natural-sized and
relatively small interval-space tensor and a "backend" that handles
mutex-protected queries and their padding and forward DFT.  This implies that a
generic, non-component +~TensorCache+ ~ThreadSafeCache~ class (using keys of shape
and values of ~ITorchTensor::pointer~) has been developed and can be reused in
future ~ITorchSpectrum~ components.


* Issues

** Shifts

A "shift" here means some relative displacement of some feature, such as a peak,
between the peak location in the filter, response or measure tensors and in the
tensor resulting from the convolution.

Categories of "shifts" considered:

- An "artificial" shift occurs when a filter or response is provided in
  artificially rolled or wrapped form.  For example, wire filters tend to be
  provided with a peak in their central sample.  Likewise, "wiener" and "gauss"
  filters, though they may be plotted on a zero-centered axis, they are often
  provided with their peak placed in the middle of their arrays.  These peaks
  are intended physically induce a "stationary" convolution.  That is, they are
  intended to broaden or narrow features in the measure and not move them.
  However, providing them in array-centered forms instead of
  zero-sample-centered forms will induce an artificial shift equal to half their
  array size.

- A "natural" shift occurs due to a filter or response truly having some peak
  that is not at zero.  For example, all FRs are peaked rather toward the high
  times (columns) in the FR tensor.  ERs are also peaked away from zero.  When
  used in deconvolution, these positive natural shifts in these responses cause
  any peaks in the measure to be shifted back to earlier times (lower column
  numbers) in the convolution result.

- A "logical" shift can occur when a measure or kernel tensor is padded.
  Padding is required in order to make convolution via DFT method be linear and
  avoid cyclic-aliasing.  The "shift" here is really about proper labeling of
  samples in channel or time space and it can include a desire to avoid having
  to consider the cyclic nature of the convolution output.

Correcting artificial shifts is straight-forward.  A natural shift in a filter
leads to shifting a measured feature so that it appears /later/ in the convolution
result.  Contrary, a natural shift in a response leads leads to shifting a
measured feature so that it appears /earlier/ in the convolution result.  Features
can appear "wrapped" in the convolution response do to its cyclical nature.  A
late measure feature can be pushed forward beyond the end of the array by a
filter shift or an early feature can be backed up before the beginning of the
array by a response shift.  Both result in the feature wrapping around to the
other end.

If we /roll/ the convolution response by the size of the response array (minus 1)
we can avoid this wrapping.  The earliest measured information shows up in
column zero of the convolution result and the latest shows up in the last
column.  The zeroth column represents "negative time", or at least time before
the first sample of the measure.

The ~ITorchSpectrum~ provides a ~shifts()~ method to communicate to the consumer the
recommended size of this roll.



